{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCELoss\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vnetpyt import vnet as Vnet\n",
    "\n",
    "from mod3DUnet import Modified3DUNet as UNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct  7 10:23:13 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\r\n",
      "| N/A   37C    P0    38W / 300W |      0MiB / 32478MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1007 10:23:15.411516 140471014573888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/niftynet/__init__.py:36: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "W1007 10:23:15.468394 140471014573888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/niftynet/utilities/util_import.py:28: The name tf.logging.fatal is deprecated. Please use tf.compat.v1.logging.fatal instead.\n",
      "\n",
      "E1007 10:23:15.470266 140471014573888 util_import.py:36] CRITICAL - Optional Python module cv2 not found, please install cv2 and retry if the application fails.\n",
      "E1007 10:23:15.471417 140471014573888 util_import.py:36] CRITICAL - Optional Python module skimage.io not found, please install skimage.io and retry if the application fails.\n",
      "W1007 10:23:15.528377 140471014573888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/niftynet/io/misc_io.py:645: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W1007 10:23:15.529419 140471014573888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/niftynet/io/misc_io.py:759: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W1007 10:23:15.530230 140471014573888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/niftynet/io/misc_io.py:759: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWARNING:niftynet:\u001b[0m From /usr/local/lib/python3.6/dist-packages/niftynet/engine/application_variables.py:20: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1007 10:23:16.212089 140471014573888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/niftynet/engine/application_variables.py:20: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWARNING:niftynet:\u001b[0m From /usr/local/lib/python3.6/dist-packages/niftynet/engine/application_variables.py:21: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1007 10:23:16.213784 140471014573888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/niftynet/engine/application_variables.py:21: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWARNING:niftynet:\u001b[0m From /usr/local/lib/python3.6/dist-packages/niftynet/engine/application_variables.py:22: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1007 10:23:16.215272 140471014573888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/niftynet/engine/application_variables.py:22: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from niftynet.engine.sampler_grid_v2 import GridSampler\n",
    "from niftynet.engine.sampler_uniform_v2 import UniformSampler\n",
    "from niftynet.engine.signal import TRAIN, VALID, INFER\n",
    "from niftynet.engine.windows_aggregator_grid import GridSamplesAggregator\n",
    "from niftynet.engine.windows_aggregator_base import ImageWindowsAggregator\n",
    "from niftynet.engine.sampler_balanced_v2 import BalancedSampler\n",
    "from niftynet.evaluation.pairwise_measures import PairwiseMeasures\n",
    "from niftynet.io.image_reader import ImageReader\n",
    "from niftynet.io.image_sets_partitioner import ImageSetsPartitioner\n",
    "from niftynet.layer.mean_variance_normalisation import MeanVarNormalisationLayer\n",
    "from niftynet.layer.pad import PadLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNiftySampler(Dataset):\n",
    "    \"\"\"\n",
    "    A simple adapter\n",
    "    converting NiftyNet sampler's output into PyTorch Dataset properties\n",
    "    \"\"\"\n",
    "    def __init__(self, sampler):\n",
    "        super(DatasetNiftySampler, self).__init__()\n",
    "        self.sampler = sampler\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.sampler(idx=index)\n",
    "\n",
    "        # Transpose to PyTorch format\n",
    "        image = np.transpose(data['image'], (0, 5, 1, 2, 3, 4))\n",
    "        label = np.transpose(data['label'], (0, 5, 1, 2, 3, 4))\n",
    "\n",
    "        image = torch.from_numpy(image).float()\n",
    "        label = torch.from_numpy(label).float()\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sampler.reader.output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDiceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, label):\n",
    "        probs = output.view(-1)\n",
    "        mask = label.view(-1)\n",
    "        smooth = 1\n",
    "        intersection = torch.sum(probs * mask)\n",
    "        den1 = torch.sum(probs)\n",
    "        den2 = torch.sum(mask)\n",
    "        soft_dice = (2 * intersection + smooth) / (den1 + den2 + smooth)\n",
    "        return -soft_dice\n",
    "\n",
    "\n",
    "def dice(input, target):\n",
    "    epsilon = 1e-8\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return 2 * intersection / (iflat.sum() + tflat.sum() + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tversky(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(tversky,self).__init__()\n",
    "        \n",
    "    def forward(self,output,label):\n",
    "        alpha = 0.1\n",
    "        beta  = 0.9\n",
    "\n",
    "        one = torch.ones_like(label)\n",
    "        p0 = output      # proba that voxels are class i\n",
    "        p1 = one-output # proba that voxels are not class i\n",
    "        g0 = label\n",
    "        g1 = one-label\n",
    "\n",
    "        num = torch.sum(p0*g0, (0,1,2,3))\n",
    "        den = num + alpha*torch.sum(p0*g1,(0,1,2,3)) + beta*torch.sum(p1*g0,(0,1,2,3))\n",
    "\n",
    "        T = torch.sum(num/den) # when summing over classes, T has dynamic range [0 Ncl]\n",
    "\n",
    "        Ncl = torch.reshape(label, (-1,)).to(dtype = torch.float32)\n",
    "        #Ncl = K.cast(K.shape(y_true)[-1], 'float32')\n",
    "        return Ncl-T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_channel_dice(input, target, epsilon=1e-5, ignore_index=None, weight=None):\n",
    "    # assumes that input is a normalized probability\n",
    "\n",
    "    # input and target shapes must match\n",
    "    assert input.size() == target.size(), \"'input' and 'target' must have the same shape\"\n",
    "\n",
    "    # mask ignore_index if present\n",
    "    if ignore_index is not None:\n",
    "        mask = target.clone().ne_(ignore_index)\n",
    "        mask.requires_grad = False\n",
    "\n",
    "        input = input * mask\n",
    "        target = target * mask\n",
    "\n",
    "    input = flatten(input)\n",
    "    target = flatten(target)\n",
    "\n",
    "    target = target.float()\n",
    "    # Compute per channel Dice Coefficient\n",
    "    intersect = (input * target).sum(-1)\n",
    "    if weight is not None:\n",
    "        intersect = weight * intersect\n",
    "\n",
    "    denominator = (input + target).sum(-1)\n",
    "    return 2. * intersect / denominator.clamp(min=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralizedDiceLoss(nn.Module):\n",
    "    \"\"\"Computes Generalized Dice Loss (GDL) as described in https://arxiv.org/pdf/1707.03237.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=1e-5, weight=None, ignore_index=None, sigmoid_normalization=True):\n",
    "        super(GeneralizedDiceLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.ignore_index = ignore_index\n",
    "        if sigmoid_normalization:\n",
    "            self.normalization = nn.Sigmoid()\n",
    "        else:\n",
    "            self.normalization = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # get probabilities from logits\n",
    "        input = self.normalization(input)\n",
    "\n",
    "        assert input.size() == target.size(), \"'input' and 'target' must have the same shape\"\n",
    "\n",
    "        # mask ignore_index if present\n",
    "        if self.ignore_index is not None:\n",
    "            mask = target.clone().ne_(self.ignore_index)\n",
    "            mask.requires_grad = False\n",
    "\n",
    "            input = input * mask\n",
    "            target = target * mask\n",
    "\n",
    "        input = torch.flatten(input)\n",
    "        target = torch.flatten(target)\n",
    "\n",
    "        target = target.float()\n",
    "        target_sum = target.sum(-1)\n",
    "        class_weights = torch.autograd.Variable(1. / (target_sum * target_sum).clamp(min=self.epsilon), requires_grad=False)\n",
    "\n",
    "        intersect = (input * target).sum(-1) * class_weights\n",
    "        if self.weight is not None:\n",
    "            weight = torch.autograd.Variable(self.weight, requires_grad=False)\n",
    "            intersect = weight * intersect\n",
    "        intersect = intersect.sum()\n",
    "\n",
    "        denominator = ((input + target).sum(-1) * class_weights).sum()\n",
    "\n",
    "        return 1. - 2. * intersect / denominator.clamp(min=self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceCoefficient:\n",
    "    \"\"\"Computes Dice Coefficient.\n",
    "    Generalized to multiple channels by computing per-channel Dice Score\n",
    "    (as described in https://arxiv.org/pdf/1707.03237.pdf) and theTn simply taking the average.\n",
    "    Input is expected to be probabilities instead of logits.\n",
    "    This metric is mostly useful when channels contain the same semantic class (e.g. affinities computed with different offsets).\n",
    "    DO NOT USE this metric when training with DiceLoss, otherwise the results will be biased towards the loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=1e-5, ignore_index=None, **kwargs):\n",
    "        self.epsilon = epsilon\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def __call__(self, input, target):\n",
    "        \"\"\"\n",
    "        :param input: 5D probability maps torch tensor (NxCxDxHxW)\n",
    "        :param target: 4D or 5D ground truth torch tensor. 4D (NxDxHxW) tensor will be expanded to 5D as one-hot\n",
    "        :return: Soft Dice Coefficient averaged over all channels/classes\n",
    "        \"\"\"\n",
    "        # Average across channels in order to get the final score\n",
    "        return torch.mean(compute_per_channel_dice(input, target, epsilon=self.epsilon, ignore_index=self.ignore_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, epsilon=1e-6):\n",
    "    y_true = y_true.cpu()\n",
    "    y_pred = y_pred.cpu()\n",
    "    y_true_flatten = np.asarray(y_true).astype(np.bool)\n",
    "    y_pred_flatten = np.asarray(y_pred).astype(np.bool)\n",
    "\n",
    "    if not np.sum(y_true_flatten) + np.sum(y_pred_flatten):\n",
    "        return 1.0\n",
    "\n",
    "    return (2. * np.sum(y_true_flatten * y_pred_flatten) + epsilon) /\\\n",
    "       (np.sum(y_true_flatten) + np.sum(y_pred_flatten) + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split_file_new = 'fcd_train_val_infer_split.csv'\n",
    "\n",
    "patch_size = (16,16,16)\n",
    "\n",
    "in_channels = 3 #3?\n",
    "\n",
    "n_classes = 1\n",
    "\n",
    "num_epochs = 150\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "ratios =[0.1,0.1]\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "cp_path = './CP2.pth'\n",
    "\n",
    "image_path = 'Fcd_Data/'\n",
    "\n",
    "label_path = 'Fcd_Data/Labels'\n",
    "\n",
    "pred_path = 'Fcd_Data/pred_brain'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reader(data_param,task_param,image_sets_partitioner, phase):\n",
    "    \n",
    "    if phase == 'training':\n",
    "        image_reader = ImageReader().initialise(\n",
    "            data_param,task_param = grouping_param,file_list=image_sets_partitioner.get_file_list(TRAIN))\n",
    "\n",
    "\n",
    "    elif phase == 'validation':\n",
    "        image_reader = ImageReader().initialise(\n",
    "            data_param,task_param = grouping_param, file_list=image_sets_partitioner.get_file_list(VALID))\n",
    "        #_, image_data, _ = image_reader(idx=0)\n",
    "\n",
    "\n",
    "    elif phase == 'inference':\n",
    "        image_reader = ImageReader().initialise(\n",
    "            data_param,task_param = grouping_param,file_list=image_sets_partitioner.get_file_list(INFER))\n",
    "        #_, image_data, _ = image_reader(idx=0)\n",
    "\n",
    "    else:\n",
    "        raise Exception('Invalid phase choice: {}'.format(\n",
    "            {'phase': ['train', 'validation', 'inference']}))\n",
    "\n",
    "    \n",
    "    mean_variance_norm_layer = MeanVarNormalisationLayer(image_name='image')\n",
    "    pad_layer = PadLayer(image_name=('image', 'label'), border=(8, 8, 8))\n",
    "    image_reader.add_preprocessing_layers([mean_variance_norm_layer])\n",
    "\n",
    "    if phase == 'inference':\n",
    "        image_reader.add_preprocessing_layers([pad_layer])\n",
    "\n",
    "    return image_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampler(image_reader, patch_size, phase):\n",
    "    if phase in ('training', 'validation'):\n",
    "        sampler = UniformSampler(image_reader,\n",
    "                                 window_sizes=patch_size,\n",
    "                                 windows_per_image=10)\n",
    "    elif phase == 'inference':\n",
    "        sampler = GridSampler(image_reader,\n",
    "                              window_sizes=patch_size,\n",
    "                              window_border=(8, 8, 8),\n",
    "                              batch_size=1)\n",
    "    else:\n",
    "        raise Exception('Invalid phase choice: {}'.format(\n",
    "            {'phase': ['train', 'validation', 'inference']}))\n",
    "\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dsets, model, optimizer,\n",
    "          num_epochs, device, cp_path, batch_size):\n",
    "    since = time.time()\n",
    "\n",
    "    dataloaders = {\n",
    "        x: DataLoader(dsets[x], batch_size=batch_size,\n",
    "                      shuffle=True, num_workers=4)\n",
    "        for x in ['training', 'validation']}\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['training', 'validation']:\n",
    "            if phase == 'training':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            ##tanay\n",
    "            running_dice = 0.0\n",
    "            \n",
    "            \n",
    "            # Iterate over data\n",
    "            for iteration, (inputs, labels) in enumerate(dataloaders[phase], 1):\n",
    "\n",
    "                nbatches, wsize, nchannels, x, y, z, _ = inputs.size()\n",
    "\n",
    "                inputs = inputs.view(nbatches * wsize, nchannels, x, y, z)\n",
    "                labels = labels.view(nbatches * wsize, 1, x, y, z)\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'training'):\n",
    "                    outputs = model(inputs)\n",
    "#                     pred = (outputs > 0.5)\n",
    "                    pred = (outputs > 0)\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'training':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                measures = PairwiseMeasures(\n",
    "                    pred.cpu().numpy(), labels.cpu().numpy())\n",
    "                \n",
    "                ##tanay\n",
    "                running_dice += dice_coef(labels,pred) * inputs.size(0)\n",
    "              \n",
    "\n",
    "            epoch_loss = running_loss / epoch_samples\n",
    "            epoch_acc = running_corrects / epoch_samples\n",
    "            epoch_dice = running_dice / epoch_samples\n",
    "\n",
    "#             print('{} Loss: {:.4f} Dice: {:.4f}'.format(\n",
    "#                 phase, epoch_loss, dice_coef(labels,pred)))\n",
    "            \n",
    "            print('{} Loss: {:.4f} Dice: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_dice))\n",
    "\n",
    "                \n",
    "\n",
    "            if epoch == 0:\n",
    "                best_loss = epoch_loss\n",
    "                torch.save(model.state_dict(), cp_path.format(epoch + 1))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'validation' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                torch.save(model.state_dict(), cp_path)\n",
    "                print('Checkpoint {} saved!'.format(epoch + 1))\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sampler, model, device, pred_path, cp_path):\n",
    "    output = GridSamplesAggregator(image_reader=sampler.reader,\n",
    "                                   window_border=(8, 8, 8),\n",
    "                                   output_path=pred_path)\n",
    "    for _ in sampler():  # for each subject\n",
    "\n",
    "        model.load_state_dict(torch.load(cp_path))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        for batch_output in sampler():  # for each sliding window step\n",
    "            window = batch_output['image']\n",
    "            # [...,0,:] eliminates time coordinate from NiftyNet Volume\n",
    "            window = window[..., 0, :]\n",
    "            window = np.transpose(window, (0, 4, 1, 2, 3))\n",
    "            window = torch.Tensor(window).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(window)\n",
    "\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            outputs = np.transpose(outputs, (0, 2, 3, 4, 1))\n",
    "            output.decode_batch({'window_image': outputs.astype(np.float32)},\n",
    "                                batch_output['image_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Reading data\n",
      "[INFO] GPU available\n",
      "\u001b[1mWARNING:niftynet:\u001b[0m From /usr/local/lib/python3.6/dist-packages/niftynet/io/image_sets_partitioner.py:269: The name tf.logging.debug is deprecated. Please use tf.compat.v1.logging.debug instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1007 10:23:16.393703 140471014573888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/niftynet/io/image_sets_partitioner.py:269: The name tf.logging.debug is deprecated. Please use tf.compat.v1.logging.debug instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWARNING:niftynet:\u001b[0m Loading from existing partitioning file fcd_train_val_infer_split.csv, ignoring partitioning ratios.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1007 10:23:16.442903 140471014573888 image_sets_partitioner.py:368] Loading from existing partitioning file fcd_train_val_infer_split.csv, ignoring partitioning ratios.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m \n",
      "\n",
      "Number of subjects 21, input section names: ['subject_id', 'modal', 'interhemi', 'pvms', 'labels']\n",
      "Dataset partitioning:\n",
      "-- training 15 cases (71.43%),\n",
      "-- validation 3 cases (14.29%),\n",
      "-- inference 3 cases (14.29%).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 10:23:16.448522 140471014573888 image_sets_partitioner.py:90] \n",
      "\n",
      "Number of subjects 21, input section names: ['subject_id', 'modal', 'interhemi', 'pvms', 'labels']\n",
      "Dataset partitioning:\n",
      "-- training 15 cases (71.43%),\n",
      "-- validation 3 cases (14.29%),\n",
      "-- inference 3 cases (14.29%).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWARNING:niftynet:\u001b[0m From /usr/local/lib/python3.6/dist-packages/niftynet/layer/base_layer.py:26: The name tf.make_template is deprecated. Please use tf.compat.v1.make_template instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1007 10:23:16.457753 140471014573888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/niftynet/layer/base_layer.py:26: The name tf.make_template is deprecated. Please use tf.compat.v1.make_template instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m Image reader: loading 15 subjects from sections ('modal', 'interhemi', 'pvms') as input [image]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 10:23:16.701769 140471014573888 image_reader.py:178] Image reader: loading 15 subjects from sections ('modal', 'interhemi', 'pvms') as input [image]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m Image reader: loading 15 subjects from sections ('labels',) as input [label]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 10:23:16.703147 140471014573888 image_reader.py:178] Image reader: loading 15 subjects from sections ('labels',) as input [label]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "reading datasets headers |----------| 0.0% \r",
      "\r",
      "reading datasets headers |***-------| 33.3% \r",
      "\r",
      "reading datasets headers |******----| 66.7% \r",
      "\u001b[1mINFO:niftynet:\u001b[0m Image reader: loading 3 subjects from sections ('modal', 'interhemi', 'pvms') as input [image]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 10:23:16.760600 140471014573888 image_reader.py:178] Image reader: loading 3 subjects from sections ('modal', 'interhemi', 'pvms') as input [image]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m Image reader: loading 3 subjects from sections ('labels',) as input [label]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 10:23:16.762017 140471014573888 image_reader.py:178] Image reader: loading 3 subjects from sections ('labels',) as input [label]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "reading datasets headers |----------| 0.0% \r",
      "\r",
      "reading datasets headers |***-------| 33.3% \r",
      "\r",
      "reading datasets headers |******----| 66.7% \r",
      "\u001b[1mINFO:niftynet:\u001b[0m Image reader: loading 3 subjects from sections ('modal', 'interhemi', 'pvms') as input [image]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 10:23:16.809921 140471014573888 image_reader.py:178] Image reader: loading 3 subjects from sections ('modal', 'interhemi', 'pvms') as input [image]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m Image reader: loading 3 subjects from sections ('labels',) as input [label]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 10:23:16.811459 140471014573888 image_reader.py:178] Image reader: loading 3 subjects from sections ('labels',) as input [label]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m initialised uniform sampler {'image': (10, 16, 16, 16, 1, 3), 'image_location': (10, 7), 'label': (10, 16, 16, 16, 1, 1), 'label_location': (10, 7)} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 10:23:16.826568 140471014573888 sampler_uniform_v2.py:45] initialised uniform sampler {'image': (10, 16, 16, 16, 1, 3), 'image_location': (10, 7), 'label': (10, 16, 16, 16, 1, 1), 'label_location': (10, 7)} \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m initialised uniform sampler {'image': (10, 16, 16, 16, 1, 3), 'image_location': (10, 7), 'label': (10, 16, 16, 16, 1, 1), 'label_location': (10, 7)} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 10:23:16.838815 140471014573888 sampler_uniform_v2.py:45] initialised uniform sampler {'image': (10, 16, 16, 16, 1, 3), 'image_location': (10, 7), 'label': (10, 16, 16, 16, 1, 1), 'label_location': (10, 7)} \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m initialised window instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 10:23:16.851155 140471014573888 sampler_grid_v2.py:53] initialised window instance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m initialised grid sampler {'image': (1, 16, 16, 16, 1, 3), 'image_location': (1, 7), 'label': (1, 16, 16, 16, 1, 1), 'label_location': (1, 7)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 10:23:16.852633 140471014573888 sampler_grid_v2.py:54] initialised grid sampler {'image': (1, 16, 16, 16, 1, 3), 'image_location': (1, 7), 'label': (1, 16, 16, 16, 1, 1), 'label_location': (1, 7)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Building model\n",
      "[INFO] Training\n",
      "Epoch 1/150\n",
      "----------\n",
      "training Loss: 0.9577 Dice: 0.0040\n",
      "validation Loss: 0.9456 Dice: 0.0027\n",
      "\n",
      "Epoch 2/150\n",
      "----------\n",
      "training Loss: 0.9459 Dice: 0.0065\n",
      "validation Loss: 0.9320 Dice: 0.0001\n",
      "Checkpoint 2 saved!\n",
      "\n",
      "Epoch 3/150\n",
      "----------\n",
      "training Loss: 0.9325 Dice: 0.0001\n",
      "validation Loss: 0.9067 Dice: 0.0027\n",
      "Checkpoint 3 saved!\n",
      "\n",
      "Epoch 4/150\n",
      "----------\n",
      "training Loss: 0.9072 Dice: 0.0120\n",
      "validation Loss: 0.8868 Dice: 0.0000\n",
      "Checkpoint 4 saved!\n",
      "\n",
      "Epoch 5/150\n",
      "----------\n",
      "training Loss: 0.8897 Dice: 0.0057\n",
      "validation Loss: 0.8711 Dice: 0.0027\n",
      "Checkpoint 5 saved!\n",
      "\n",
      "Epoch 6/150\n",
      "----------\n",
      "training Loss: 0.8726 Dice: 0.0007\n",
      "validation Loss: 0.8543 Dice: 0.0000\n",
      "Checkpoint 6 saved!\n",
      "\n",
      "Epoch 7/150\n",
      "----------\n",
      "training Loss: 0.8573 Dice: 0.0002\n",
      "validation Loss: 0.8431 Dice: 0.0027\n",
      "Checkpoint 7 saved!\n",
      "\n",
      "Epoch 8/150\n",
      "----------\n",
      "training Loss: 0.8455 Dice: 0.0009\n",
      "validation Loss: 0.8321 Dice: 0.0027\n",
      "Checkpoint 8 saved!\n",
      "\n",
      "Epoch 9/150\n",
      "----------\n",
      "training Loss: 0.8319 Dice: 0.0007\n",
      "validation Loss: 0.8211 Dice: 0.0001\n",
      "Checkpoint 9 saved!\n",
      "\n",
      "Epoch 10/150\n",
      "----------\n",
      "training Loss: 0.8236 Dice: 0.0002\n",
      "validation Loss: 0.8131 Dice: 0.0027\n",
      "Checkpoint 10 saved!\n",
      "\n",
      "Epoch 11/150\n",
      "----------\n",
      "training Loss: 0.8146 Dice: 0.0003\n",
      "validation Loss: 0.8044 Dice: 0.0001\n",
      "Checkpoint 11 saved!\n",
      "\n",
      "Epoch 12/150\n",
      "----------\n",
      "training Loss: 0.8061 Dice: 0.0000\n",
      "validation Loss: 0.7976 Dice: 0.0001\n",
      "Checkpoint 12 saved!\n",
      "\n",
      "Epoch 13/150\n",
      "----------\n",
      "training Loss: 0.7980 Dice: 0.0000\n",
      "validation Loss: 0.7900 Dice: 0.0000\n",
      "Checkpoint 13 saved!\n",
      "\n",
      "Epoch 14/150\n",
      "----------\n",
      "training Loss: 0.7923 Dice: 0.0000\n",
      "validation Loss: 0.7853 Dice: 0.0000\n",
      "Checkpoint 14 saved!\n",
      "\n",
      "Epoch 15/150\n",
      "----------\n",
      "training Loss: 0.7871 Dice: 0.0002\n",
      "validation Loss: 0.7794 Dice: 0.0027\n",
      "Checkpoint 15 saved!\n",
      "\n",
      "Epoch 16/150\n",
      "----------\n",
      "training Loss: 0.7812 Dice: 0.0010\n",
      "validation Loss: 0.7764 Dice: 0.0027\n",
      "Checkpoint 16 saved!\n",
      "\n",
      "Epoch 17/150\n",
      "----------\n",
      "training Loss: 0.7770 Dice: 0.0002\n",
      "validation Loss: 0.7706 Dice: 0.0027\n",
      "Checkpoint 17 saved!\n",
      "\n",
      "Epoch 18/150\n",
      "----------\n",
      "training Loss: 0.7715 Dice: 0.0125\n",
      "validation Loss: 0.7673 Dice: 0.0000\n",
      "Checkpoint 18 saved!\n",
      "\n",
      "Epoch 19/150\n",
      "----------\n",
      "training Loss: 0.7678 Dice: 0.0133\n",
      "validation Loss: 0.7630 Dice: 0.0000\n",
      "Checkpoint 19 saved!\n",
      "\n",
      "Epoch 20/150\n",
      "----------\n",
      "training Loss: 0.7649 Dice: 0.0013\n",
      "validation Loss: 0.7604 Dice: 0.0000\n",
      "Checkpoint 20 saved!\n",
      "\n",
      "Epoch 21/150\n",
      "----------\n",
      "training Loss: 0.7611 Dice: 0.0002\n",
      "validation Loss: 0.7573 Dice: 0.0000\n",
      "Checkpoint 21 saved!\n",
      "\n",
      "Epoch 22/150\n",
      "----------\n",
      "training Loss: 0.7580 Dice: 0.0001\n",
      "validation Loss: 0.7538 Dice: 0.0027\n",
      "Checkpoint 22 saved!\n",
      "\n",
      "Epoch 23/150\n",
      "----------\n",
      "training Loss: 0.7551 Dice: 0.0001\n",
      "validation Loss: 0.7513 Dice: 0.0027\n",
      "Checkpoint 23 saved!\n",
      "\n",
      "Epoch 24/150\n",
      "----------\n",
      "training Loss: 0.7515 Dice: 0.0135\n",
      "validation Loss: 0.7497 Dice: 0.0027\n",
      "Checkpoint 24 saved!\n",
      "\n",
      "Epoch 25/150\n",
      "----------\n",
      "training Loss: 0.7499 Dice: 0.0000\n",
      "validation Loss: 0.7472 Dice: 0.0000\n",
      "Checkpoint 25 saved!\n",
      "\n",
      "Epoch 26/150\n",
      "----------\n",
      "training Loss: 0.7471 Dice: 0.0011\n",
      "validation Loss: 0.7446 Dice: 0.0027\n",
      "Checkpoint 26 saved!\n",
      "\n",
      "Epoch 27/150\n",
      "----------\n",
      "training Loss: 0.7453 Dice: 0.0036\n",
      "validation Loss: 0.7427 Dice: 0.0027\n",
      "Checkpoint 27 saved!\n",
      "\n",
      "Epoch 28/150\n",
      "----------\n",
      "training Loss: 0.7433 Dice: 0.0001\n",
      "validation Loss: 0.7415 Dice: 0.0027\n",
      "Checkpoint 28 saved!\n",
      "\n",
      "Epoch 29/150\n",
      "----------\n",
      "training Loss: 0.7416 Dice: 0.0002\n",
      "validation Loss: 0.7397 Dice: 0.0027\n",
      "Checkpoint 29 saved!\n",
      "\n",
      "Epoch 30/150\n",
      "----------\n",
      "training Loss: 0.7399 Dice: 0.0000\n",
      "validation Loss: 0.7375 Dice: 0.0000\n",
      "Checkpoint 30 saved!\n",
      "\n",
      "Epoch 31/150\n",
      "----------\n",
      "training Loss: 0.7379 Dice: 0.0028\n",
      "validation Loss: 0.7360 Dice: 0.0000\n",
      "Checkpoint 31 saved!\n",
      "\n",
      "Epoch 32/150\n",
      "----------\n",
      "training Loss: 0.7368 Dice: 0.0024\n",
      "validation Loss: 0.7350 Dice: 0.0001\n",
      "Checkpoint 32 saved!\n",
      "\n",
      "Epoch 33/150\n",
      "----------\n",
      "training Loss: 0.7351 Dice: 0.0026\n",
      "validation Loss: 0.7333 Dice: 0.0027\n",
      "Checkpoint 33 saved!\n",
      "\n",
      "Epoch 34/150\n",
      "----------\n",
      "training Loss: 0.7340 Dice: 0.0006\n",
      "validation Loss: 0.7323 Dice: 0.0000\n",
      "Checkpoint 34 saved!\n",
      "\n",
      "Epoch 35/150\n",
      "----------\n",
      "training Loss: 0.7327 Dice: 0.0000\n",
      "validation Loss: 0.7313 Dice: 0.0027\n",
      "Checkpoint 35 saved!\n",
      "\n",
      "Epoch 36/150\n",
      "----------\n",
      "training Loss: 0.7314 Dice: 0.0002\n",
      "validation Loss: 0.7297 Dice: 0.0027\n",
      "Checkpoint 36 saved!\n",
      "\n",
      "Epoch 37/150\n",
      "----------\n",
      "training Loss: 0.7297 Dice: 0.0173\n",
      "validation Loss: 0.7289 Dice: 0.0000\n",
      "Checkpoint 37 saved!\n",
      "\n",
      "Epoch 38/150\n",
      "----------\n",
      "training Loss: 0.7288 Dice: 0.0072\n",
      "validation Loss: 0.7279 Dice: 0.0001\n",
      "Checkpoint 38 saved!\n",
      "\n",
      "Epoch 39/150\n",
      "----------\n",
      "training Loss: 0.7281 Dice: 0.0024\n",
      "validation Loss: 0.7269 Dice: 0.0000\n",
      "Checkpoint 39 saved!\n",
      "\n",
      "Epoch 40/150\n",
      "----------\n",
      "training Loss: 0.7271 Dice: 0.0049\n",
      "validation Loss: 0.7261 Dice: 0.0027\n",
      "Checkpoint 40 saved!\n",
      "\n",
      "Epoch 41/150\n",
      "----------\n",
      "training Loss: 0.7262 Dice: 0.0000\n",
      "validation Loss: 0.7250 Dice: 0.0000\n",
      "Checkpoint 41 saved!\n",
      "\n",
      "Epoch 42/150\n",
      "----------\n",
      "training Loss: 0.7252 Dice: 0.0066\n",
      "validation Loss: 0.7241 Dice: 0.0027\n",
      "Checkpoint 42 saved!\n",
      "\n",
      "Epoch 43/150\n",
      "----------\n",
      "training Loss: 0.7246 Dice: 0.0013\n",
      "validation Loss: 0.7235 Dice: 0.0000\n",
      "Checkpoint 43 saved!\n",
      "\n",
      "Epoch 44/150\n",
      "----------\n",
      "training Loss: 0.7236 Dice: 0.0011\n",
      "validation Loss: 0.7226 Dice: 0.0027\n",
      "Checkpoint 44 saved!\n",
      "\n",
      "Epoch 45/150\n",
      "----------\n",
      "training Loss: 0.7227 Dice: 0.0138\n",
      "validation Loss: 0.7220 Dice: 0.0000\n",
      "Checkpoint 45 saved!\n",
      "\n",
      "Epoch 46/150\n",
      "----------\n",
      "training Loss: 0.7221 Dice: 0.0000\n",
      "validation Loss: 0.7214 Dice: 0.0001\n",
      "Checkpoint 46 saved!\n",
      "\n",
      "Epoch 47/150\n",
      "----------\n",
      "training Loss: 0.7215 Dice: 0.0010\n",
      "validation Loss: 0.7207 Dice: 0.0001\n",
      "Checkpoint 47 saved!\n",
      "\n",
      "Epoch 48/150\n",
      "----------\n",
      "training Loss: 0.7209 Dice: 0.0000\n",
      "validation Loss: 0.7201 Dice: 0.0000\n",
      "Checkpoint 48 saved!\n",
      "\n",
      "Epoch 49/150\n",
      "----------\n",
      "training Loss: 0.7200 Dice: 0.0068\n",
      "validation Loss: 0.7195 Dice: 0.0001\n",
      "Checkpoint 49 saved!\n",
      "\n",
      "Epoch 50/150\n",
      "----------\n",
      "training Loss: 0.7197 Dice: 0.0000\n",
      "validation Loss: 0.7188 Dice: 0.0000\n",
      "Checkpoint 50 saved!\n",
      "\n",
      "Epoch 51/150\n",
      "----------\n",
      "training Loss: 0.7190 Dice: 0.0000\n",
      "validation Loss: 0.7182 Dice: 0.0027\n",
      "Checkpoint 51 saved!\n",
      "\n",
      "Epoch 52/150\n",
      "----------\n",
      "training Loss: 0.7184 Dice: 0.0001\n",
      "validation Loss: 0.7178 Dice: 0.0000\n",
      "Checkpoint 52 saved!\n",
      "\n",
      "Epoch 53/150\n",
      "----------\n",
      "training Loss: 0.7178 Dice: 0.0000\n",
      "validation Loss: 0.7173 Dice: 0.0001\n",
      "Checkpoint 53 saved!\n",
      "\n",
      "Epoch 54/150\n",
      "----------\n",
      "training Loss: 0.7175 Dice: 0.0010\n",
      "validation Loss: 0.7167 Dice: 0.0000\n",
      "Checkpoint 54 saved!\n",
      "\n",
      "Epoch 55/150\n",
      "----------\n",
      "training Loss: 0.7168 Dice: 0.0047\n",
      "validation Loss: 0.7163 Dice: 0.0001\n",
      "Checkpoint 55 saved!\n",
      "\n",
      "Epoch 56/150\n",
      "----------\n",
      "training Loss: 0.7160 Dice: 0.0187\n",
      "validation Loss: 0.7159 Dice: 0.0027\n",
      "Checkpoint 56 saved!\n",
      "\n",
      "Epoch 57/150\n",
      "----------\n",
      "training Loss: 0.7158 Dice: 0.0085\n",
      "validation Loss: 0.7154 Dice: 0.0000\n",
      "Checkpoint 57 saved!\n",
      "\n",
      "Epoch 58/150\n",
      "----------\n",
      "training Loss: 0.7152 Dice: 0.0109\n",
      "validation Loss: 0.7148 Dice: 0.0027\n",
      "Checkpoint 58 saved!\n",
      "\n",
      "Epoch 59/150\n",
      "----------\n",
      "training Loss: 0.7151 Dice: 0.0002\n",
      "validation Loss: 0.7146 Dice: 0.0000\n",
      "Checkpoint 59 saved!\n",
      "\n",
      "Epoch 60/150\n",
      "----------\n",
      "training Loss: 0.7146 Dice: 0.0000\n",
      "validation Loss: 0.7142 Dice: 0.0027\n",
      "Checkpoint 60 saved!\n",
      "\n",
      "Epoch 61/150\n",
      "----------\n",
      "training Loss: 0.7142 Dice: 0.0008\n",
      "validation Loss: 0.7138 Dice: 0.0001\n",
      "Checkpoint 61 saved!\n",
      "\n",
      "Epoch 62/150\n",
      "----------\n",
      "training Loss: 0.7139 Dice: 0.0016\n",
      "validation Loss: 0.7134 Dice: 0.0001\n",
      "Checkpoint 62 saved!\n",
      "\n",
      "Epoch 63/150\n",
      "----------\n",
      "training Loss: 0.7134 Dice: 0.0077\n",
      "validation Loss: 0.7131 Dice: 0.0001\n",
      "Checkpoint 63 saved!\n",
      "\n",
      "Epoch 64/150\n",
      "----------\n",
      "training Loss: 0.7131 Dice: 0.0000\n",
      "validation Loss: 0.7126 Dice: 0.0000\n",
      "Checkpoint 64 saved!\n",
      "\n",
      "Epoch 65/150\n",
      "----------\n",
      "training Loss: 0.7128 Dice: 0.0008\n",
      "validation Loss: 0.7124 Dice: 0.0000\n",
      "Checkpoint 65 saved!\n",
      "\n",
      "Epoch 66/150\n",
      "----------\n",
      "training Loss: 0.7125 Dice: 0.0001\n",
      "validation Loss: 0.7119 Dice: 0.0027\n",
      "Checkpoint 66 saved!\n",
      "\n",
      "Epoch 67/150\n",
      "----------\n",
      "training Loss: 0.7121 Dice: 0.0011\n",
      "validation Loss: 0.7116 Dice: 0.0027\n",
      "Checkpoint 67 saved!\n",
      "\n",
      "Epoch 68/150\n",
      "----------\n",
      "training Loss: 0.7118 Dice: 0.0000\n",
      "validation Loss: 0.7114 Dice: 0.0000\n",
      "Checkpoint 68 saved!\n",
      "\n",
      "Epoch 69/150\n",
      "----------\n",
      "training Loss: 0.7114 Dice: 0.0037\n",
      "validation Loss: 0.7112 Dice: 0.0027\n",
      "Checkpoint 69 saved!\n",
      "\n",
      "Epoch 70/150\n",
      "----------\n",
      "training Loss: 0.7112 Dice: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.7109 Dice: 0.0000\n",
      "Checkpoint 70 saved!\n",
      "\n",
      "Epoch 71/150\n",
      "----------\n",
      "training Loss: 0.7109 Dice: 0.0002\n",
      "validation Loss: 0.7106 Dice: 0.0001\n",
      "Checkpoint 71 saved!\n",
      "\n",
      "Epoch 72/150\n",
      "----------\n",
      "training Loss: 0.7103 Dice: 0.0132\n",
      "validation Loss: 0.7103 Dice: 0.0027\n",
      "Checkpoint 72 saved!\n",
      "\n",
      "Epoch 73/150\n",
      "----------\n",
      "training Loss: 0.7103 Dice: 0.0053\n",
      "validation Loss: 0.7099 Dice: 0.0027\n",
      "Checkpoint 73 saved!\n",
      "\n",
      "Epoch 74/150\n",
      "----------\n",
      "training Loss: 0.7100 Dice: 0.0082\n",
      "validation Loss: 0.7098 Dice: 0.0000\n",
      "Checkpoint 74 saved!\n",
      "\n",
      "Epoch 75/150\n",
      "----------\n",
      "training Loss: 0.7098 Dice: 0.0000\n",
      "validation Loss: 0.7095 Dice: 0.0000\n",
      "Checkpoint 75 saved!\n",
      "\n",
      "Epoch 76/150\n",
      "----------\n",
      "training Loss: 0.7096 Dice: 0.0000\n",
      "validation Loss: 0.7093 Dice: 0.0027\n",
      "Checkpoint 76 saved!\n",
      "\n",
      "Epoch 77/150\n",
      "----------\n",
      "training Loss: 0.7093 Dice: 0.0027\n",
      "validation Loss: 0.7090 Dice: 0.0000\n",
      "Checkpoint 77 saved!\n",
      "\n",
      "Epoch 78/150\n",
      "----------\n",
      "training Loss: 0.7091 Dice: 0.0026\n",
      "validation Loss: 0.7088 Dice: 0.0000\n",
      "Checkpoint 78 saved!\n",
      "\n",
      "Epoch 79/150\n",
      "----------\n",
      "training Loss: 0.7088 Dice: 0.0000\n",
      "validation Loss: 0.7085 Dice: 0.0027\n",
      "Checkpoint 79 saved!\n",
      "\n",
      "Epoch 80/150\n",
      "----------\n",
      "training Loss: 0.7086 Dice: 0.0001\n",
      "validation Loss: 0.7084 Dice: 0.0027\n",
      "Checkpoint 80 saved!\n",
      "\n",
      "Epoch 81/150\n",
      "----------\n",
      "training Loss: 0.7084 Dice: 0.0000\n",
      "validation Loss: 0.7081 Dice: 0.0000\n",
      "Checkpoint 81 saved!\n",
      "\n",
      "Epoch 82/150\n",
      "----------\n",
      "training Loss: 0.7080 Dice: 0.0132\n",
      "validation Loss: 0.7080 Dice: 0.0001\n",
      "Checkpoint 82 saved!\n",
      "\n",
      "Epoch 83/150\n",
      "----------\n",
      "training Loss: 0.7080 Dice: 0.0001\n",
      "validation Loss: 0.7077 Dice: 0.0000\n",
      "Checkpoint 83 saved!\n",
      "\n",
      "Epoch 84/150\n",
      "----------\n",
      "training Loss: 0.7076 Dice: 0.0133\n",
      "validation Loss: 0.7075 Dice: 0.0001\n",
      "Checkpoint 84 saved!\n",
      "\n",
      "Epoch 85/150\n",
      "----------\n",
      "training Loss: 0.7076 Dice: 0.0001\n",
      "validation Loss: 0.7074 Dice: 0.0027\n",
      "Checkpoint 85 saved!\n",
      "\n",
      "Epoch 86/150\n",
      "----------\n",
      "training Loss: 0.7074 Dice: 0.0001\n",
      "validation Loss: 0.7072 Dice: 0.0000\n",
      "Checkpoint 86 saved!\n",
      "\n",
      "Epoch 87/150\n",
      "----------\n",
      "training Loss: 0.7071 Dice: 0.0006\n",
      "validation Loss: 0.7069 Dice: 0.0027\n",
      "Checkpoint 87 saved!\n",
      "\n",
      "Epoch 88/150\n",
      "----------\n",
      "training Loss: 0.7070 Dice: 0.0009\n",
      "validation Loss: 0.7067 Dice: 0.0027\n",
      "Checkpoint 88 saved!\n",
      "\n",
      "Epoch 89/150\n",
      "----------\n",
      "training Loss: 0.7069 Dice: 0.0000\n",
      "validation Loss: 0.7066 Dice: 0.0027\n",
      "Checkpoint 89 saved!\n",
      "\n",
      "Epoch 90/150\n",
      "----------\n",
      "training Loss: 0.7066 Dice: 0.0002\n",
      "validation Loss: 0.7064 Dice: 0.0027\n",
      "Checkpoint 90 saved!\n",
      "\n",
      "Epoch 91/150\n",
      "----------\n",
      "training Loss: 0.7065 Dice: 0.0032\n",
      "validation Loss: 0.7063 Dice: 0.0027\n",
      "Checkpoint 91 saved!\n",
      "\n",
      "Epoch 92/150\n",
      "----------\n",
      "training Loss: 0.7063 Dice: 0.0035\n",
      "validation Loss: 0.7061 Dice: 0.0001\n",
      "Checkpoint 92 saved!\n",
      "\n",
      "Epoch 93/150\n",
      "----------\n",
      "training Loss: 0.7062 Dice: 0.0001\n",
      "validation Loss: 0.7059 Dice: 0.0000\n",
      "Checkpoint 93 saved!\n",
      "\n",
      "Epoch 94/150\n",
      "----------\n",
      "training Loss: 0.7060 Dice: 0.0000\n",
      "validation Loss: 0.7058 Dice: 0.0001\n",
      "Checkpoint 94 saved!\n",
      "\n",
      "Epoch 95/150\n",
      "----------\n",
      "training Loss: 0.7058 Dice: 0.0000\n",
      "validation Loss: 0.7056 Dice: 0.0000\n",
      "Checkpoint 95 saved!\n",
      "\n",
      "Epoch 96/150\n",
      "----------\n",
      "training Loss: 0.7056 Dice: 0.0000\n",
      "validation Loss: 0.7054 Dice: 0.0027\n",
      "Checkpoint 96 saved!\n",
      "\n",
      "Epoch 97/150\n",
      "----------\n",
      "training Loss: 0.7055 Dice: 0.0022\n",
      "validation Loss: 0.7053 Dice: 0.0027\n",
      "Checkpoint 97 saved!\n",
      "\n",
      "Epoch 98/150\n",
      "----------\n",
      "training Loss: 0.7054 Dice: 0.0002\n",
      "validation Loss: 0.7052 Dice: 0.0000\n",
      "Checkpoint 98 saved!\n",
      "\n",
      "Epoch 99/150\n",
      "----------\n",
      "training Loss: 0.7052 Dice: 0.0003\n",
      "validation Loss: 0.7050 Dice: 0.0001\n",
      "Checkpoint 99 saved!\n",
      "\n",
      "Epoch 100/150\n",
      "----------\n",
      "training Loss: 0.7050 Dice: 0.0027\n",
      "validation Loss: 0.7049 Dice: 0.0001\n",
      "Checkpoint 100 saved!\n",
      "\n",
      "Epoch 101/150\n",
      "----------\n",
      "training Loss: 0.7050 Dice: 0.0003\n",
      "validation Loss: 0.7048 Dice: 0.0027\n",
      "Checkpoint 101 saved!\n",
      "\n",
      "Epoch 102/150\n",
      "----------\n",
      "training Loss: 0.7046 Dice: 0.0135\n",
      "validation Loss: 0.7046 Dice: 0.0000\n",
      "Checkpoint 102 saved!\n",
      "\n",
      "Epoch 103/150\n",
      "----------\n",
      "training Loss: 0.7047 Dice: 0.0000\n",
      "validation Loss: 0.7044 Dice: 0.0027\n",
      "Checkpoint 103 saved!\n",
      "\n",
      "Epoch 104/150\n",
      "----------\n",
      "training Loss: 0.7045 Dice: 0.0009\n",
      "validation Loss: 0.7043 Dice: 0.0027\n",
      "Checkpoint 104 saved!\n",
      "\n",
      "Epoch 105/150\n",
      "----------\n",
      "training Loss: 0.7044 Dice: 0.0001\n",
      "validation Loss: 0.7042 Dice: 0.0000\n",
      "Checkpoint 105 saved!\n",
      "\n",
      "Epoch 106/150\n",
      "----------\n",
      "training Loss: 0.7043 Dice: 0.0004\n",
      "validation Loss: 0.7040 Dice: 0.0027\n",
      "Checkpoint 106 saved!\n",
      "\n",
      "Epoch 107/150\n",
      "----------\n",
      "training Loss: 0.7041 Dice: 0.0070\n",
      "validation Loss: 0.7039 Dice: 0.0000\n",
      "Checkpoint 107 saved!\n",
      "\n",
      "Epoch 108/150\n",
      "----------\n",
      "training Loss: 0.7040 Dice: 0.0033\n",
      "validation Loss: 0.7039 Dice: 0.0001\n",
      "Checkpoint 108 saved!\n",
      "\n",
      "Epoch 109/150\n",
      "----------\n",
      "training Loss: 0.7037 Dice: 0.0132\n",
      "validation Loss: 0.7037 Dice: 0.0000\n",
      "Checkpoint 109 saved!\n",
      "\n",
      "Epoch 110/150\n",
      "----------\n",
      "training Loss: 0.7038 Dice: 0.0001\n",
      "validation Loss: 0.7037 Dice: 0.0027\n",
      "Checkpoint 110 saved!\n",
      "\n",
      "Epoch 111/150\n",
      "----------\n",
      "training Loss: 0.7036 Dice: 0.0001\n",
      "validation Loss: 0.7035 Dice: 0.0000\n",
      "Checkpoint 111 saved!\n",
      "\n",
      "Epoch 112/150\n",
      "----------\n",
      "training Loss: 0.7036 Dice: 0.0002\n",
      "validation Loss: 0.7034 Dice: 0.0027\n",
      "Checkpoint 112 saved!\n",
      "\n",
      "Epoch 113/150\n",
      "----------\n",
      "training Loss: 0.7034 Dice: 0.0000\n",
      "validation Loss: 0.7033 Dice: 0.0000\n",
      "Checkpoint 113 saved!\n",
      "\n",
      "Epoch 114/150\n",
      "----------\n",
      "training Loss: 0.7034 Dice: 0.0002\n",
      "validation Loss: 0.7031 Dice: 0.0027\n",
      "Checkpoint 114 saved!\n",
      "\n",
      "Epoch 115/150\n",
      "----------\n",
      "training Loss: 0.7031 Dice: 0.0140\n",
      "validation Loss: 0.7031 Dice: 0.0001\n",
      "Checkpoint 115 saved!\n",
      "\n",
      "Epoch 116/150\n",
      "----------\n",
      "training Loss: 0.7031 Dice: 0.0000\n",
      "validation Loss: 0.7030 Dice: 0.0001\n",
      "Checkpoint 116 saved!\n",
      "\n",
      "Epoch 117/150\n",
      "----------\n",
      "training Loss: 0.7030 Dice: 0.0003\n",
      "validation Loss: 0.7028 Dice: 0.0000\n",
      "Checkpoint 117 saved!\n",
      "\n",
      "Epoch 118/150\n",
      "----------\n",
      "training Loss: 0.7029 Dice: 0.0003\n",
      "validation Loss: 0.7028 Dice: 0.0000\n",
      "Checkpoint 118 saved!\n",
      "\n",
      "Epoch 119/150\n",
      "----------\n",
      "training Loss: 0.7028 Dice: 0.0006\n",
      "validation Loss: 0.7027 Dice: 0.0027\n",
      "Checkpoint 119 saved!\n",
      "\n",
      "Epoch 120/150\n",
      "----------\n",
      "training Loss: 0.7027 Dice: 0.0065\n",
      "validation Loss: 0.7026 Dice: 0.0027\n",
      "Checkpoint 120 saved!\n",
      "\n",
      "Epoch 121/150\n",
      "----------\n",
      "training Loss: 0.7026 Dice: 0.0006\n",
      "validation Loss: 0.7024 Dice: 0.0027\n",
      "Checkpoint 121 saved!\n",
      "\n",
      "Epoch 122/150\n",
      "----------\n",
      "training Loss: 0.7025 Dice: 0.0022\n",
      "validation Loss: 0.7024 Dice: 0.0000\n",
      "Checkpoint 122 saved!\n",
      "\n",
      "Epoch 123/150\n",
      "----------\n",
      "training Loss: 0.7024 Dice: 0.0015\n",
      "validation Loss: 0.7023 Dice: 0.0000\n",
      "Checkpoint 123 saved!\n",
      "\n",
      "Epoch 124/150\n",
      "----------\n",
      "training Loss: 0.7023 Dice: 0.0003\n",
      "validation Loss: 0.7022 Dice: 0.0000\n",
      "Checkpoint 124 saved!\n",
      "\n",
      "Epoch 125/150\n",
      "----------\n",
      "training Loss: 0.7022 Dice: 0.0000\n",
      "validation Loss: 0.7021 Dice: 0.0000\n",
      "Checkpoint 125 saved!\n",
      "\n",
      "Epoch 126/150\n",
      "----------\n",
      "training Loss: 0.7021 Dice: 0.0003\n",
      "validation Loss: 0.7020 Dice: 0.0000\n",
      "Checkpoint 126 saved!\n",
      "\n",
      "Epoch 127/150\n",
      "----------\n",
      "training Loss: 0.7020 Dice: 0.0016\n",
      "validation Loss: 0.7019 Dice: 0.0000\n",
      "Checkpoint 127 saved!\n",
      "\n",
      "Epoch 128/150\n",
      "----------\n",
      "training Loss: 0.7018 Dice: 0.0138\n",
      "validation Loss: 0.7019 Dice: 0.0000\n",
      "Checkpoint 128 saved!\n",
      "\n",
      "Epoch 129/150\n",
      "----------\n",
      "training Loss: 0.7019 Dice: 0.0000\n",
      "validation Loss: 0.7017 Dice: 0.0027\n",
      "Checkpoint 129 saved!\n",
      "\n",
      "Epoch 130/150\n",
      "----------\n",
      "training Loss: 0.7018 Dice: 0.0011\n",
      "validation Loss: 0.7017 Dice: 0.0001\n",
      "Checkpoint 130 saved!\n",
      "\n",
      "Epoch 131/150\n",
      "----------\n",
      "training Loss: 0.7017 Dice: 0.0000\n",
      "validation Loss: 0.7016 Dice: 0.0000\n",
      "Checkpoint 131 saved!\n",
      "\n",
      "Epoch 132/150\n",
      "----------\n",
      "training Loss: 0.7016 Dice: 0.0027\n",
      "validation Loss: 0.7016 Dice: 0.0027\n",
      "Checkpoint 132 saved!\n",
      "\n",
      "Epoch 133/150\n",
      "----------\n",
      "training Loss: 0.7016 Dice: 0.0000\n",
      "validation Loss: 0.7015 Dice: 0.0027\n",
      "Checkpoint 133 saved!\n",
      "\n",
      "Epoch 134/150\n",
      "----------\n",
      "training Loss: 0.7015 Dice: 0.0026\n",
      "validation Loss: 0.7014 Dice: 0.0000\n",
      "Checkpoint 134 saved!\n",
      "\n",
      "Epoch 135/150\n",
      "----------\n",
      "training Loss: 0.7014 Dice: 0.0001\n",
      "validation Loss: 0.7013 Dice: 0.0001\n",
      "Checkpoint 135 saved!\n",
      "\n",
      "Epoch 136/150\n",
      "----------\n",
      "training Loss: 0.7013 Dice: 0.0002\n",
      "validation Loss: 0.7012 Dice: 0.0000\n",
      "Checkpoint 136 saved!\n",
      "\n",
      "Epoch 137/150\n",
      "----------\n",
      "training Loss: 0.7013 Dice: 0.0001\n",
      "validation Loss: 0.7011 Dice: 0.0027\n",
      "Checkpoint 137 saved!\n",
      "\n",
      "Epoch 138/150\n",
      "----------\n",
      "training Loss: 0.7012 Dice: 0.0002\n",
      "validation Loss: 0.7010 Dice: 0.0027\n",
      "Checkpoint 138 saved!\n",
      "\n",
      "Epoch 139/150\n",
      "----------\n",
      "training Loss: 0.7011 Dice: 0.0014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.7010 Dice: 0.0000\n",
      "Checkpoint 139 saved!\n",
      "\n",
      "Epoch 140/150\n",
      "----------\n",
      "training Loss: 0.7010 Dice: 0.0000\n",
      "validation Loss: 0.7010 Dice: 0.0027\n",
      "Checkpoint 140 saved!\n",
      "\n",
      "Epoch 141/150\n",
      "----------\n",
      "training Loss: 0.7010 Dice: 0.0006\n",
      "validation Loss: 0.7008 Dice: 0.0000\n",
      "Checkpoint 141 saved!\n",
      "\n",
      "Epoch 142/150\n",
      "----------\n",
      "training Loss: 0.7009 Dice: 0.0001\n",
      "validation Loss: 0.7008 Dice: 0.0001\n",
      "Checkpoint 142 saved!\n",
      "\n",
      "Epoch 143/150\n",
      "----------\n",
      "training Loss: 0.7008 Dice: 0.0000\n",
      "validation Loss: 0.7008 Dice: 0.0027\n",
      "Checkpoint 143 saved!\n",
      "\n",
      "Epoch 144/150\n",
      "----------\n",
      "training Loss: 0.7008 Dice: 0.0002\n",
      "validation Loss: 0.7007 Dice: 0.0001\n",
      "Checkpoint 144 saved!\n",
      "\n",
      "Epoch 145/150\n",
      "----------\n",
      "training Loss: 0.7007 Dice: 0.0000\n",
      "validation Loss: 0.7006 Dice: 0.0000\n",
      "Checkpoint 145 saved!\n",
      "\n",
      "Epoch 146/150\n",
      "----------\n",
      "training Loss: 0.7006 Dice: 0.0026\n",
      "validation Loss: 0.7006 Dice: 0.0027\n",
      "Checkpoint 146 saved!\n",
      "\n",
      "Epoch 147/150\n",
      "----------\n",
      "training Loss: 0.7005 Dice: 0.0132\n",
      "validation Loss: 0.7005 Dice: 0.0000\n",
      "Checkpoint 147 saved!\n",
      "\n",
      "Epoch 148/150\n",
      "----------\n",
      "training Loss: 0.7005 Dice: 0.0000\n",
      "validation Loss: 0.7004 Dice: 0.0000\n",
      "Checkpoint 148 saved!\n",
      "\n",
      "Epoch 149/150\n",
      "----------\n",
      "training Loss: 0.7004 Dice: 0.0027\n",
      "validation Loss: 0.7004 Dice: 0.0000\n",
      "Checkpoint 149 saved!\n",
      "\n",
      "Epoch 150/150\n",
      "----------\n",
      "training Loss: 0.7002 Dice: 0.0169\n",
      "validation Loss: 0.7003 Dice: 0.0027\n",
      "Checkpoint 150 saved!\n",
      "\n",
      "Training complete in 115m 56s\n",
      "[INFO] Inference\n",
      "\u001b[1mINFO:niftynet:\u001b[0m grid sampling image sizes: {'image': (224, 272, 272, 1, 3), 'label': (224, 272, 272, 1, 1)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 12:19:19.589962 140471014573888 sampler_grid_v2.py:77] grid sampling image sizes: {'image': (224, 272, 272, 1, 3), 'label': (224, 272, 272, 1, 1)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m grid sampling window sizes: {'image': (16, 16, 16, 1, 3), 'label': (16, 16, 16, 1, 1)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 12:19:19.591950 140471014573888 sampler_grid_v2.py:79] grid sampling window sizes: {'image': (16, 16, 16, 1, 3), 'label': (16, 16, 16, 1, 1)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m yielding 13804241 locations from image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 12:19:19.593578 140471014573888 sampler_grid_v2.py:87] yielding 13804241 locations from image\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m grid sampling image sizes: {'image': (368, 272, 272, 1, 3), 'label': (368, 272, 272, 1, 1)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 12:19:30.636020 140471014573888 sampler_grid_v2.py:77] grid sampling image sizes: {'image': (368, 272, 272, 1, 3), 'label': (368, 272, 272, 1, 1)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m grid sampling window sizes: {'image': (16, 16, 16, 1, 3), 'label': (16, 16, 16, 1, 1)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 12:19:30.637915 140471014573888 sampler_grid_v2.py:79] grid sampling window sizes: {'image': (16, 16, 16, 1, 3), 'label': (16, 16, 16, 1, 1)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO:niftynet:\u001b[0m yielding 23315297 locations from image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1007 12:19:30.639634 140471014573888 sampler_grid_v2.py:87] yielding 23315297 locations from image\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-afb7bfa046e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] Inference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inference'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcp_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-edabd5df0fee>\u001b[0m in \u001b[0;36minference\u001b[0;34m(sampler, model, device, pred_path, cp_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             output.decode_batch({'window_image': outputs.astype(np.float32)},\n\u001b[0;32m---> 24\u001b[0;31m                                 batch_output['image_location'])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/niftynet/engine/windows_aggregator_grid.py\u001b[0m in \u001b[0;36mdecode_batch\u001b[0;34m(self, window, location)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_border\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/niftynet/engine/windows_aggregator_base.py\u001b[0m in \u001b[0;36mcrop_batch\u001b[0;34m(window, location, border)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mwindow_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mspatial_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mn_spatial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspatial_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(\"[INFO]Reading data\")\n",
    "    # Dictionary with data parameters for NiftyNet Reader\n",
    "if torch.cuda.is_available():\n",
    "    print('[INFO] GPU available')\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "else:\n",
    "    raise Exception(\"[INFO] No GPU found or Wrong gpu id\")\n",
    "\n",
    "    # Dictionary with data parameters for NiftyNet Reader\n",
    "data_param = {\n",
    "        'modal': {'path_to_search': os.path.join(image_path, 'images'), 'filename_contains': 'reg'},\n",
    "        'interhemi': {'path_to_search': os.path.join(image_path, 'Interhemi'), 'filename_contains': 'interasymm'},\n",
    "        'pvms': {'path_to_search': os.path.join(image_path, 'Pvms'), 'filename_contains' : 'pve'},\n",
    "        'labels': {'path_to_search': label_path, 'filename_contains': 'label'}}\n",
    "\n",
    "grouping_param = {'image': ('modal', 'interhemi', 'pvms'), \n",
    "                  'label': ('labels',)}\n",
    "\n",
    "\n",
    "image_sets_partitioner = ImageSetsPartitioner().initialise(\n",
    "        data_param=data_param,\n",
    "        data_split_file=data_split_file_new,\n",
    "        new_partition=False,\n",
    "        ratios = ratios)\n",
    "\n",
    "readers = {x: get_reader(data_param,grouping_param,image_sets_partitioner, x) for x in ['training', 'validation', 'inference']}\n",
    "samplers = {x: get_sampler(readers[x], patch_size, x) for x in ['training', 'validation', 'inference']}\n",
    "\n",
    "    # Training stage only\n",
    "dsets = {x: DatasetNiftySampler(sampler=samplers[x])\n",
    "             for x in ['training', 'validation']}\n",
    "\n",
    "print(\"[INFO] Building model\")\n",
    "#model = models.resnet18(pretrained=False)\n",
    "model = UNet(in_channels,n_classes)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print(\"[INFO] Training\")\n",
    "train(dsets, model,optimizer,num_epochs, device, cp_path, batch_size)\n",
    "\n",
    "print(\"[INFO] Inference\")\n",
    "inference(samplers['inference'], model, device, pred_path, cp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
